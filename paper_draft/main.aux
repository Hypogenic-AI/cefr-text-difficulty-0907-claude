\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{council2001common}
\citation{arase2022cefr,khallaf2021automatic}
\citation{arase2022cefr}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{vajjala2012improving}
\citation{pilan2016predicting}
\citation{xia2016text}
\citation{arnold2018predicting}
\citation{khallaf2021automatic}
\citation{arase2022cefr}
\citation{lagutina2023comparison}
\citation{fujinuma2021joint}
\citation{imperial2025universalcefr}
\citation{lagutina2023comparison,khallaf2021automatic,arnold2018predicting}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\citation{hale2001probabilistic,levy2008expectation}
\citation{pitler2008revisiting}
\citation{xia2016text,francois2012ai}
\citation{arase2022cefr}
\citation{radford2019language}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Class distribution of the \textsc  {CEFR-SP}\xspace  English dataset.}}{3}{table.1}\protected@file@percent }
\newlabel{tab:data-dist}{{1}{3}{Class distribution of the \cefrsp English dataset}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:dataset}{{3.1}{3}{Dataset}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feature Extraction}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:features}{{3.2}{3}{Feature Extraction}{subsection.3.2}{}}
\citation{chen2016xgboost}
\citation{devlin2019bert}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Interpretable Classifiers}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:classifiers}{{3.3}{4}{Interpretable Classifiers}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Feature Group Ablation}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:ablation}{{3.4}{4}{Feature Group Ablation}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}BERT Fine-Tuning}{4}{subsection.3.5}\protected@file@percent }
\newlabel{sec:bert}{{3.5}{4}{BERT Fine-Tuning}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Diagnostic Experiments}{4}{subsection.3.6}\protected@file@percent }
\newlabel{sec:diagnostics}{{3.6}{4}{Diagnostic Experiments}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Evaluation Metrics}{4}{subsection.3.7}\protected@file@percent }
\newlabel{sec:metrics}{{3.7}{4}{Evaluation Metrics}{subsection.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Correlations with \textsc  {CEFR}\xspace  Level}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:correlations}{{4.1}{4}{Feature Correlations with \cefr Level}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Classifier Performance}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:clf-results}{{4.2}{4}{Classifier Performance}{subsection.4.2}{}}
\citation{lagutina2023comparison,khallaf2021automatic}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Top 10 features by Spearman correlation ($\rho $) with ordinal \textsc  {CEFR}\xspace  level. All correlations are significant at $p < 0.001$.}}{5}{table.2}\protected@file@percent }
\newlabel{tab:top-features}{{2}{5}{Top 10 features by Spearman correlation ($\rho $) with ordinal \cefr level. All correlations are significant at $p < 0.001$}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Classifier performance (mean $\pm $ std over 5 folds). Best results in \textbf  {bold}.}}{5}{table.3}\protected@file@percent }
\newlabel{tab:model-comparison}{{3}{5}{Classifier performance (mean $\pm $ std over 5 folds). Best results in \textbf {bold}}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Feature Group Ablation}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:ablation-results}{{4.3}{5}{Feature Group Ablation}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Feature Importance}{5}{subsection.4.4}\protected@file@percent }
\newlabel{sec:importance}{{4.4}{5}{Feature Importance}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Feature group ablation using \textsc  {XGBoost}\xspace  . \emph  {Individual}: trained on one group only. \emph  {Leave-one-out}: trained on all groups except one. The drop column shows the F$_1$\xspace  decrease from the full model (0.435).}}{6}{table.4}\protected@file@percent }
\newlabel{tab:ablation}{{4}{6}{Feature group ablation using \xgb . \emph {Individual}: trained on one group only. \emph {Leave-one-out}: trained on all groups except one. The drop column shows the \fone decrease from the full model (0.435)}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {XGBoost}\xspace  feature importance (gain). \textsc  {ARI}\xspace  dominates at 18\% of total importance. Surprisal features appear in the top 10 despite weak standalone performance, confirming their complementary value.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:importance}{{1}{6}{\xgb feature importance (gain). \ari dominates at 18\% of total importance. Surprisal features appear in the top 10 despite weak standalone performance, confirming their complementary value}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\textsc  {BERT}\xspace  Probing}{6}{subsection.4.5}\protected@file@percent }
\newlabel{sec:probing}{{4.5}{6}{\bert Probing}{subsection.4.5}{}}
\citation{vajjala2012improving,pilan2016predicting}
\citation{xia2016text}
\citation{pitler2008revisiting}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textsc  {BERT}\xspace  probing $R^2$: variance in \textsc  {BERT}\xspace  's predicted class explained by each feature group (ridge regression, 5-fold CV).}}{7}{table.5}\protected@file@percent }
\newlabel{tab:probing}{{5}{7}{\bert probing $R^2$: variance in \bert 's predicted class explained by each feature group (ridge regression, 5-fold CV)}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Prediction agreement between \textsc  {XGBoost}\xspace  and \textsc  {BERT}\xspace  (10,004 samples).}}{7}{table.6}\protected@file@percent }
\newlabel{tab:agreement}{{6}{7}{Prediction agreement between \xgb and \bert (10,004 samples)}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Error Analysis}{7}{subsection.4.6}\protected@file@percent }
\newlabel{sec:errors}{{4.6}{7}{Error Analysis}{subsection.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{7}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}What Makes Sentences Difficult?}{7}{subsection.5.1}\protected@file@percent }
\newlabel{sec:what-makes-difficult}{{5.1}{7}{What Makes Sentences Difficult?}{subsection.5.1}{}}
\citation{pilan2016predicting}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Per-level F$_1$\xspace  and accuracy for \textsc  {XGBoost}\xspace  and \textsc  {BERT}\xspace  . $\Delta $ shows \textsc  {BERT}\xspace  's accuracy advantage. Best per-level F$_1$\xspace  in \textbf  {bold}.}}{8}{table.7}\protected@file@percent }
\newlabel{tab:per-level}{{7}{8}{Per-level \fone and accuracy for \xgb and \bert . $\Delta $ shows \bert 's accuracy advantage. Best per-level \fone in \textbf {bold}}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Per-class F$_1$\xspace  for \textsc  {XGBoost}\xspace  and \textsc  {BERT}\xspace  . \textsc  {BERT}\xspace  outperforms on all levels except A1 ($n\!=\!124$), with the largest gains at the boundary levels A2 and C1.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:per-class}{{2}{8}{Per-class \fone for \xgb and \bert . \bert outperforms on all levels except A1 ($n\!=\!124$), with the largest gains at the boundary levels A2 and C1}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}What Does \textsc  {BERT}\xspace  Know That Features Miss?}{8}{subsection.5.2}\protected@file@percent }
\newlabel{sec:bert-advantage}{{5.2}{8}{What Does \bert Know That Features Miss?}{subsection.5.2}{}}
\citation{pilan2016predicting}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Implications for Practitioners}{9}{subsection.5.3}\protected@file@percent }
\newlabel{sec:implications}{{5.3}{9}{Implications for Practitioners}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Limitations}{9}{subsection.5.4}\protected@file@percent }
\newlabel{sec:limitations}{{5.4}{9}{Limitations}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{9}{Conclusion}{section.6}{}}
\bibdata{references}
\bibcite{arase2022cefr}{{1}{2022}{{Arase et~al.}}{{Arase, Uchida, and Kajiwara}}}
\bibcite{arnold2018predicting}{{2}{2018}{{Arnold et~al.}}{{Arnold, Ballier, Gaillat, and Liss{\'o}n}}}
\bibcite{chen2016xgboost}{{3}{2016}{{Chen and Guestrin}}{{}}}
\bibcite{council2001common}{{4}{2001}{{Council of Europe}}{{}}}
\bibcite{devlin2019bert}{{5}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{francois2012ai}{{6}{2012}{{Fran{\c {c}}ois and Fairon}}{{}}}
\bibcite{fujinuma2021joint}{{7}{2021}{{Fujinuma and Hagiwara}}{{}}}
\bibcite{hale2001probabilistic}{{8}{2001}{{Hale}}{{}}}
\bibcite{imperial2025universalcefr}{{9}{2025}{{Imperial et~al.}}{{Imperial, Tack, Francois, and Shardlow}}}
\bibcite{khallaf2021automatic}{{10}{2021}{{Khallaf and Sharoff}}{{}}}
\bibcite{lagutina2023comparison}{{11}{2023}{{Lagutina et~al.}}{{Lagutina, Lagutina, Boychuk, and Nikiforova}}}
\bibcite{levy2008expectation}{{12}{2008}{{Levy}}{{}}}
\bibcite{pilan2016predicting}{{13}{2016}{{Pil{\'a}n et~al.}}{{Pil{\'a}n, Volodina, and Zesch}}}
\bibcite{pitler2008revisiting}{{14}{2008}{{Pitler and Nenkova}}{{}}}
\bibcite{radford2019language}{{15}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{vajjala2012improving}{{16}{2012}{{Vajjala and Meurers}}{{}}}
\bibcite{xia2016text}{{17}{2016}{{Xia et~al.}}{{Xia, Kochmar, and Briscoe}}}
\bibstyle{plainnat}
\gdef \@abspage@last{11}
